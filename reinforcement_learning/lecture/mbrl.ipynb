{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "from itertools import product\n",
    "from typing import Tuple\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "rng_key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 2\n",
    "num_actions = 2\n",
    "states = [0, 1]\n",
    "actions = [0, 1]\n",
    "\n",
    "\n",
    "def mdp(state: int, action: int) -> tuple[int, float]:\n",
    "    assert state in [0, 1], \"Invalid state\"\n",
    "    assert action in [0, 1], \"Invalid action\"\n",
    "\n",
    "    if state == 0:\n",
    "        if action == 0:\n",
    "            next_state = 0 if np.random.rand() < 0.9 else 1\n",
    "            reward = np.random.normal(2.0, 1.0)\n",
    "            return next_state, reward\n",
    "        elif action == 1:\n",
    "            next_state = 0 if np.random.rand() < 0.1 else 1\n",
    "            reward = np.random.normal(1.5, 1.0)\n",
    "            return next_state, reward\n",
    "    elif state == 1:\n",
    "        if action == 0:\n",
    "            next_state = 0 if np.random.rand() < 0.9 else 1\n",
    "            reward = np.random.normal(0.0, 1.0)\n",
    "            return next_state, reward\n",
    "        elif action == 1:\n",
    "            next_state = 0 if np.random.rand() < 0.2 else 1\n",
    "            reward = np.random.normal(3.0, 1.0)\n",
    "            return next_state, reward\n",
    "    assert False, \"Should not reach here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 1, Action: 1, Reward: 3.5626485439432516\n",
      "State: 1, Action: 0, Reward: 0.08674049946635698\n",
      "State: 0, Action: 1, Reward: 1.5155929367283145\n",
      "State: 1, Action: 1, Reward: 2.642991390603359\n",
      "State: 1, Action: 1, Reward: 4.050187765651558\n",
      "State: 1, Action: 1, Reward: 4.735093173729242\n",
      "State: 1, Action: 1, Reward: 2.3329309020785756\n",
      "State: 1, Action: 0, Reward: 0.5269864672580259\n",
      "State: 0, Action: 0, Reward: 0.5124401925578577\n",
      "State: 0, Action: 0, Reward: 0.38451923945558564\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Sample a trajectory\n",
    "s = np.random.choice(states)\n",
    "a = np.random.choice(actions)\n",
    "state_traj = [s]\n",
    "action_traj = [a]\n",
    "reward_traj = []\n",
    "\n",
    "for _ in range(300):\n",
    "    s_, reward = mdp(s, a)\n",
    "    a_ = np.random.choice(actions)\n",
    "\n",
    "    state_traj.append(s_)\n",
    "    action_traj.append(a_)\n",
    "    reward_traj.append(reward)\n",
    "    s = s_\n",
    "    a = a_\n",
    "\n",
    "for s, a, r in list(zip(state_traj, action_traj, reward_traj))[:10]:\n",
    "    print(f\"State: {s}, Action: {a}, Reward: {r}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdp_model(states, actions, rewards, next_states):\n",
    "    # Prior for transition probabilities for each state-action pair\n",
    "    alpha_prior = jnp.array([\n",
    "        [[1.0, 1.0], [1.0, 1.0]],\n",
    "        [[1.0, 1.0], [1.0, 1.0]]\n",
    "    ])\n",
    "\n",
    "    trans_probs = numpyro.sample(\n",
    "        \"trans_probs\",\n",
    "        dist.Dirichlet(alpha_prior).expand((num_states, num_actions))\n",
    "    )\n",
    "\n",
    "    # Prior for reward means\n",
    "    reward_means = numpyro.sample(\n",
    "        \"reward_means\",\n",
    "        dist.Normal(0, 5.0).expand((num_states, num_actions))\n",
    "    )\n",
    "\n",
    "    reward_stds = numpyro.sample(\n",
    "        \"reward_stds\",\n",
    "        dist.HalfNormal(5.0).expand((num_states, num_actions))\n",
    "    )\n",
    "\n",
    "    with numpyro.plate(\"record\", len(states)):\n",
    "        numpyro.sample(\n",
    "            \"trans\",\n",
    "            dist.Categorical(probs=trans_probs[states, actions]),\n",
    "            obs=next_states\n",
    "        )\n",
    "\n",
    "        # Gaussian likelihood for rewards\n",
    "        numpyro.sample(\n",
    "            \"reward\",\n",
    "            dist.Normal(\n",
    "                reward_means[states, actions],\n",
    "                reward_stds[states, actions]\n",
    "            ),\n",
    "            obs=rewards\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state_traj = jnp.array(state_traj[1:])\n",
    "state_traj = jnp.array(state_traj[:300])\n",
    "action_traj = jnp.array(action_traj[:300])\n",
    "reward_traj = jnp.array(reward_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 3000/3000 [00:03<00:00, 834.83it/s, 7 steps of size 5.52e-01. acc. prob=0.92] \n"
     ]
    }
   ],
   "source": [
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "kernel = NUTS(mdp_model)\n",
    "mcmc = MCMC(kernel, num_warmup=1000, num_samples=2000)\n",
    "mcmc.run(subkey, state_traj, action_traj, reward_traj, next_state_traj)\n",
    "samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probabilities:\n",
      "P(s' | s=0, a=0) = [0.92480195 0.07519802]\n",
      "P(s' | s=0, a=1) = [0.0983525 0.9016475]\n",
      "P(s' | s=1, a=0) = [0.9293374  0.07066277]\n",
      "P(s' | s=1, a=1) = [0.2576761 0.7423239]\n",
      "Reward means:\n",
      "E[r | s=0, a=0] = 1.7359533309936523\n",
      "E[r | s=0, a=1] = 1.5451616048812866\n",
      "E[r | s=1, a=0] = -0.21869699656963348\n",
      "E[r | s=1, a=1] = 2.837351083755493\n"
     ]
    }
   ],
   "source": [
    "trans_probs_mean = jnp.mean(samples['trans_probs'], axis=0)\n",
    "reward_means_mean = jnp.mean(samples['reward_means'], axis=0)\n",
    "\n",
    "# Print the learned transition probabilities\n",
    "print(\"Transition probabilities:\")\n",
    "for s, a in product(states, actions):\n",
    "    print(f\"P(s' | s={s}, a={a}) = {trans_probs_mean[s, a]}\")\n",
    "\n",
    "# Print the learned reward means\n",
    "print(\"Reward means:\")\n",
    "for s, a in product(states, actions):\n",
    "    print(f\"E[r | s={s}, a={a}] = {reward_means_mean[s, a]}\")\n",
    "\n",
    "# Transition probabilities:\n",
    "# P(s' | s=0, a=0) = [0.87283117 0.12716895]\n",
    "# P(s' | s=0, a=1) = [0.08410239 0.9158976 ]\n",
    "# P(s' | s=1, a=0) = [0.804974   0.19502601]\n",
    "# P(s' | s=1, a=1) = [0.14632195 0.85367817]\n",
    "# Reward means:\n",
    "# E[r | s=0, a=0] = 2.0038230419158936\n",
    "# E[r | s=0, a=1] = 1.5468661785125732\n",
    "# E[r | s=1, a=0] = -0.12734664976596832\n",
    "# E[r | s=1, a=1] = 2.970562696456909\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(\n",
    "        policy: NDArray[np.float64],\n",
    "        P: NDArray[np.float64],\n",
    "        R: NDArray[np.float64],\n",
    "        gamma: float = 0.95,\n",
    "        theta: float = 0.01,\n",
    "        max_iter: int = 1000\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"Perform policy evaluation to estimate the state value function with a given policy.\n",
    "    \"\"\"\n",
    "    num_states = P.shape[0]\n",
    "    num_actions = P.shape[1]\n",
    "    assert policy.shape == (num_states, num_actions)\n",
    "\n",
    "    V = np.zeros(num_states)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        delta = 0\n",
    "        V_new = np.zeros(num_states)\n",
    "        for s in range(num_states):\n",
    "            for a in range(num_actions):\n",
    "                for s_next in range(num_states):\n",
    "                    V_new[s] += policy[s, a] * P[s, a, s_next] * (R[s, a] + gamma * V[s_next])\n",
    "            delta = max(delta, np.abs(V_new[s] - V[s]))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "        V = V_new\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(\n",
    "        V: NDArray[np.float64],\n",
    "        P: NDArray[np.float64],\n",
    "        R: NDArray[np.float64],\n",
    "        gamma=0.99\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"Perform policy improvement to find the optimal policy given the value function.\"\"\"\n",
    "    num_states = P.shape[0]\n",
    "    num_actions = P.shape[1]\n",
    "    policy = np.zeros((num_states, num_actions))\n",
    "\n",
    "    for s in range(num_states):\n",
    "        Q = np.zeros(num_actions)\n",
    "        for a in range(num_actions):\n",
    "            for s_next in range(num_states):\n",
    "                Q[a] += P[s, a, s_next] * (R[s, a] + gamma * V[s_next])\n",
    "\n",
    "        ai = np.argmax(Q)\n",
    "        policy[s, ai] = 1.0\n",
    "\n",
    "    assert np.allclose(np.sum(policy, axis=1), 1.0)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(\n",
    "        P: NDArray[np.float64],\n",
    "        R: NDArray[np.float64],\n",
    "        gamma: float = 0.99\n",
    ") -> Tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "    \"\"\"Perform policy iteration to find the optimal policy and value function.\"\"\"\n",
    "    num_states = P.shape[0]\n",
    "    num_actions = P.shape[1]\n",
    "    policy = np.zeros((num_states, num_actions))\n",
    "    policy[:, 0] = 1.0  # Start with a policy that always takes action 0\n",
    "\n",
    "    while True:\n",
    "        V = policy_evaluation(policy, P, R, gamma)\n",
    "        new_policy = policy_improvement(V, P, R, gamma)\n",
    "\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            break\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      " [[0. 1.]\n",
      " [0. 1.]]\n",
      "Optimal Value Function: [270.62017394 271.9850511 ]\n"
     ]
    }
   ],
   "source": [
    "# Run policy iteration\n",
    "P = np.zeros((2, 2, 2))  # [state, action, next_state]\n",
    "\n",
    "# State 0. Using the true transition probabilities for simplicity\n",
    "P[0, 0, 0] = 0.9  # Action 0: 90% chance to stay in state 0\n",
    "P[0, 0, 1] = 0.1  # Action 0: 10% chance to go to state 1\n",
    "P[0, 1, 0] = 0.1  # Action 1: 10% chance to stay in state 0\n",
    "P[0, 1, 1] = 0.9  # Action 1: 90% chance to go to state 1\n",
    "\n",
    "# State 1\n",
    "P[1, 0, 0] = 0.9  # Action 0: 90% chance to go to state 0\n",
    "P[1, 0, 1] = 0.1  # Action 0: 10% chance to stay in state 1\n",
    "P[1, 1, 0] = 0.2  # Action 1: 20% chance to go to state 0\n",
    "P[1, 1, 1] = 0.8  # Action 1: 80% chance to stay in state 1\n",
    "\n",
    "R = np.zeros((2, 2))  # [state, action]\n",
    "\n",
    "# State 0\n",
    "R[0, 0] = 2.0  # Expected reward for action 0\n",
    "R[0, 1] = 1.5  # Expected reward for action 1\n",
    "\n",
    "# State 1\n",
    "R[1, 0] = 0.0  # Expected reward for action 0\n",
    "R[1, 1] = 3.0  # Expected reward for action 1\n",
    "optimal_policy, optimal_value = policy_iteration(P, R)\n",
    "print(\"Optimal Policy:\\n\", optimal_policy)\n",
    "print(\"Optimal Value Function:\", optimal_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
