{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25452b9d",
   "metadata": {},
   "source": [
    "# Finding the Optimal Policy with Linear Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa247e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48efca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Dialogue MDP Solution via Linear Programming\n",
      "============================================================\n",
      "\n",
      "Optimal Value Function:\n",
      "  V(S0      ) =   5.9969\n",
      "  V(W       ) =   6.5927\n",
      "  V(M       ) =   6.5927\n",
      "  V(WM      ) =   7.5000\n",
      "  V(SUCCESS ) =   0.0000\n",
      "  V(ABANDON ) =   0.0000\n",
      "\n",
      "Optimal Policy:\n",
      "  π(S0      ) = ASK_BOTH      [Q-values: ASK_WEATHER=5.786, ASK_MOOD=5.756, ASK_BOTH=5.997]\n",
      "  π(W       ) = ASK_MOOD      [Q-values: ASK_MOOD=6.593]\n",
      "  π(M       ) = ASK_WEATHER   [Q-values: ASK_WEATHER=6.593]\n",
      "  π(WM      ) = RECOMMEND     [Q-values: RECOMMEND=7.500]\n",
      "\n",
      "Solver Status: ok\n",
      "============================================================\n",
      "\n",
      "Interpretation:\n",
      "  Starting from S0, the expected total reward is 5.9969\n",
      "  Since reward is +10 for SUCCESS, this means P(SUCCESS) ≈ 59.97%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Solve the Dialogue/Recommendation MDP using Linear Programming with Pyomo.\n",
    "\n",
    "MDP Setup:\n",
    "- States: S0 (start), W (have weather), M (have mood), WM (have both), SUCCESS, ABANDON\n",
    "- Actions: ASK_WEATHER, ASK_MOOD, ASK_BOTH, RECOMMEND (availability depends on state)\n",
    "- Reward: +10 for reaching SUCCESS, 0 otherwise\n",
    "- Terminal states: SUCCESS, ABANDON (no discounting needed, but we use γ < 1 for numerical stability)\n",
    "\n",
    "LP Formulation:\n",
    "    Minimize: Σ V(s)  (over non-terminal states)\n",
    "    Subject to: V(s) ≥ Σ P(s'|s,a) * [R(s,a,s') + γ*V(s')]  for all valid (s,a) pairs\n",
    "    \n",
    "    Terminal states have V = 0 (no future rewards possible)\n",
    "\"\"\"\n",
    "\n",
    "import pyomo.environ as pyo\n",
    "from enum import IntEnum\n",
    "\n",
    "\n",
    "# ----- MDP Definition (from the user's code) -----\n",
    "class State(IntEnum):\n",
    "    S0 = 0\n",
    "    W = 1\n",
    "    M = 2\n",
    "    WM = 3\n",
    "    SUCCESS = 4\n",
    "    ABANDON = 5\n",
    "\n",
    "\n",
    "class Action(IntEnum):\n",
    "    ASK_WEATHER = 0\n",
    "    ASK_MOOD = 1\n",
    "    ASK_BOTH = 2\n",
    "    RECOMMEND = 3\n",
    "\n",
    "\n",
    "TERMINAL_STATES = {State.SUCCESS, State.ABANDON}\n",
    "NON_TERMINAL_STATES = [s for s in State if s not in TERMINAL_STATES]\n",
    "\n",
    "AVAILABLE_ACTIONS = {\n",
    "    State.S0: [Action.ASK_WEATHER, Action.ASK_MOOD, Action.ASK_BOTH],\n",
    "    State.W: [Action.ASK_MOOD],\n",
    "    State.M: [Action.ASK_WEATHER],\n",
    "    State.WM: [Action.RECOMMEND],\n",
    "    State.SUCCESS: [],\n",
    "    State.ABANDON: [],\n",
    "}\n",
    "\n",
    "TRANSITIONS = {\n",
    "    (State.S0, Action.ASK_WEATHER): {\n",
    "        State.W: 0.75,\n",
    "        State.S0: 0.15,\n",
    "        State.ABANDON: 0.10,\n",
    "    },\n",
    "    (State.S0, Action.ASK_MOOD): {\n",
    "        State.M: 0.70,\n",
    "        State.S0: 0.20,\n",
    "        State.ABANDON: 0.10,\n",
    "    },\n",
    "    (State.S0, Action.ASK_BOTH): {\n",
    "        State.WM: 0.50,\n",
    "        State.W: 0.20,\n",
    "        State.M: 0.15,\n",
    "        State.ABANDON: 0.15,\n",
    "    },\n",
    "    (State.W, Action.ASK_MOOD): {\n",
    "        State.WM: 0.80,\n",
    "        State.W: 0.10,\n",
    "        State.ABANDON: 0.10,\n",
    "    },\n",
    "    (State.M, Action.ASK_WEATHER): {\n",
    "        State.WM: 0.80,\n",
    "        State.M: 0.10,\n",
    "        State.ABANDON: 0.10,\n",
    "    },\n",
    "    (State.WM, Action.RECOMMEND): {\n",
    "        State.SUCCESS: 0.75,\n",
    "        State.ABANDON: 0.25,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def reward(state, action, next_state):\n",
    "    \"\"\"Reward is +10 for reaching SUCCESS, 0 otherwise.\"\"\"\n",
    "    return 10.0 if next_state == State.SUCCESS else 0.0\n",
    "\n",
    "\n",
    "# ----- Pyomo Model -----\n",
    "gamma = 0.99  # Discount factor (close to 1 since we care about reaching SUCCESS)\n",
    "\n",
    "model = pyo.ConcreteModel(\"Dialogue_MDP_LP\")\n",
    "\n",
    "# Decision variables: V(s) for each state\n",
    "model.V = pyo.Var(list(State), domain=pyo.Reals)\n",
    "\n",
    "# Fix terminal state values to 0\n",
    "model.terminal_success = pyo.Constraint(expr=model.V[State.SUCCESS] == 0)\n",
    "model.terminal_abandon = pyo.Constraint(expr=model.V[State.ABANDON] == 0)\n",
    "\n",
    "# Objective: minimize sum of values (over non-terminal states)\n",
    "model.obj = pyo.Objective(\n",
    "    expr=sum(model.V[s] for s in NON_TERMINAL_STATES),\n",
    "    sense=pyo.minimize\n",
    ")\n",
    "\n",
    "# Constraints: V(s) >= Σ P(s'|s,a) * [R(s,a,s') + γ*V(s')]  for all valid (s,a)\n",
    "model.bellman = pyo.ConstraintList()\n",
    "\n",
    "for s in NON_TERMINAL_STATES:\n",
    "    for a in AVAILABLE_ACTIONS[s]:\n",
    "        trans = TRANSITIONS[(s, a)]\n",
    "        \n",
    "        # Expected immediate reward + discounted future value\n",
    "        rhs = sum(\n",
    "            prob * (reward(s, a, s_next) + gamma * model.V[s_next])\n",
    "            for s_next, prob in trans.items()\n",
    "        )\n",
    "        \n",
    "        model.bellman.add(model.V[s] >= rhs)\n",
    "\n",
    "# ----- Solve -----\n",
    "solver = pyo.SolverFactory('ipopt')\n",
    "result = solver.solve(model, tee=False)\n",
    "\n",
    "# ----- Results -----\n",
    "print(\"=\" * 60)\n",
    "print(\"Dialogue MDP Solution via Linear Programming\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "for s in State:\n",
    "    print(f\"  V({s.name:8}) = {pyo.value(model.V[s]):8.4f}\")\n",
    "\n",
    "# Extract optimal policy\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for s in NON_TERMINAL_STATES:\n",
    "    v_s = pyo.value(model.V[s])\n",
    "    best_action = None\n",
    "    best_q = float('-inf')\n",
    "    \n",
    "    action_values = []\n",
    "    for a in AVAILABLE_ACTIONS[s]:\n",
    "        trans = TRANSITIONS[(s, a)]\n",
    "        q_value = sum(\n",
    "            prob * (reward(s, a, s_next) + gamma * pyo.value(model.V[s_next]))\n",
    "            for s_next, prob in trans.items()\n",
    "        )\n",
    "        action_values.append((a, q_value))\n",
    "        if q_value > best_q:\n",
    "            best_q = q_value\n",
    "            best_action = a\n",
    "    \n",
    "    # Show all Q-values for this state\n",
    "    q_str = \", \".join(f\"{a.name}={q:.3f}\" for a, q in action_values)\n",
    "    print(f\"  π({s.name:8}) = {best_action.name:12}  [Q-values: {q_str}]\")\n",
    "\n",
    "print(f\"\\nSolver Status: {result.solver.status}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ----- Verify: Expected reward from S0 -----\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"  Starting from S0, the expected total reward is {pyo.value(model.V[State.S0]):.4f}\")\n",
    "print(f\"  Since reward is +10 for SUCCESS, this means P(SUCCESS) ≈ {pyo.value(model.V[State.S0])/10:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bfa47e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
