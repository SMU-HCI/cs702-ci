{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25452b9d",
   "metadata": {},
   "source": [
    "# Finding the Optimal Policy with Linear Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa247e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "\n",
    "# MDP from the previous lecture.\n",
    "class State(IntEnum):\n",
    "    S0 = 0\n",
    "    W = 1\n",
    "    M = 2\n",
    "    WM = 3\n",
    "    SUCCESS = 4\n",
    "    ABANDON = 5\n",
    "\n",
    "\n",
    "class Action(IntEnum):\n",
    "    ASK_WEATHER = 0\n",
    "    ASK_MOOD = 1\n",
    "    ASK_BOTH = 2\n",
    "    RECOMMEND = 3\n",
    "\n",
    "\n",
    "TERMINAL_STATES = {State.SUCCESS, State.ABANDON}\n",
    "NON_TERMINAL_STATES = [s for s in State if s not in TERMINAL_STATES]\n",
    "\n",
    "AVAILABLE_ACTIONS = {\n",
    "    State.S0: [Action.ASK_WEATHER, Action.ASK_MOOD, Action.ASK_BOTH],\n",
    "    State.W: [Action.ASK_MOOD],\n",
    "    State.M: [Action.ASK_WEATHER],\n",
    "    State.WM: [Action.RECOMMEND],\n",
    "    State.SUCCESS: [],\n",
    "    State.ABANDON: [],\n",
    "}\n",
    "\n",
    "TRANSITIONS = {\n",
    "    (State.S0, Action.ASK_WEATHER): {\n",
    "        State.W: 0.75,\n",
    "        State.S0: 0.15,\n",
    "        State.ABANDON: 0.10,\n",
    "    },\n",
    "    (State.S0, Action.ASK_MOOD): {\n",
    "        State.M: 0.70,\n",
    "        State.S0: 0.20,\n",
    "        State.ABANDON: 0.10,\n",
    "    },\n",
    "    (State.S0, Action.ASK_BOTH): {\n",
    "        State.WM: 0.50,\n",
    "        State.W: 0.20,\n",
    "        State.M: 0.15,\n",
    "        State.ABANDON: 0.15,\n",
    "    },\n",
    "    (State.W, Action.ASK_MOOD): {\n",
    "        State.WM: 0.80,\n",
    "        State.W: 0.10,\n",
    "        State.ABANDON: 0.10,\n",
    "    },\n",
    "    (State.M, Action.ASK_WEATHER): {\n",
    "        State.WM: 0.80,\n",
    "        State.M: 0.10,\n",
    "        State.ABANDON: 0.10,\n",
    "    },\n",
    "    (State.WM, Action.RECOMMEND): {\n",
    "        State.SUCCESS: 0.75,\n",
    "        State.ABANDON: 0.25,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def reward(state, action, next_state):\n",
    "    \"\"\"Reward is +10 for reaching SUCCESS, 0 otherwise.\"\"\"\n",
    "    return 10.0 if next_state == State.SUCCESS else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48efca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyomo.environ as pyo\n",
    "\n",
    "\n",
    "# Decision variables V(s) for each state\n",
    "model = pyo.ConcreteModel(\"Dialogue_MDP_LP\")\n",
    "model.V = pyo.Var(list(State), domain=pyo.Reals)\n",
    "\n",
    "# Fix terminal state values to 0\n",
    "model.terminal_success = pyo.Constraint(expr=model.V[State.SUCCESS] == 0)\n",
    "model.terminal_abandon = pyo.Constraint(expr=model.V[State.ABANDON] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bfa47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective: minimize sum of values over non-terminal states\n",
    "model.obj = pyo.Objective(\n",
    "    expr=sum(model.V[s] for s in NON_TERMINAL_STATES), sense=pyo.minimize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Bellman Constraint\n",
    "model.bellman = pyo.ConstraintList()\n",
    "#gamma = 0.99  # Discount factor\n",
    "gamma = 1.0  # No discounting\n",
    "\n",
    "for s in NON_TERMINAL_STATES:\n",
    "    for a in AVAILABLE_ACTIONS[s]:\n",
    "        trans = TRANSITIONS[(s, a)]\n",
    "\n",
    "        # Expected immediate reward + discounted future value\n",
    "        rhs = sum(\n",
    "            prob * (reward(s, a, s_next) + gamma * model.V[s_next])\n",
    "            for s_next, prob in trans.items()\n",
    "        )\n",
    "\n",
    "        model.bellman.add(model.V[s] >= rhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d530166",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = pyo.SolverFactory(\"ipopt\")\n",
    "result = solver.solve(model, tee=False)\n",
    "\n",
    "print(\"Optimal Value Function:\")\n",
    "for s in State:\n",
    "    print(f\"  V({s.name}) = {pyo.value(model.V[s]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fa1852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract optimal policy\n",
    "print(\"Optimal Policy:\")\n",
    "for s in NON_TERMINAL_STATES:\n",
    "    v_s = pyo.value(model.V[s])\n",
    "    best_action = None\n",
    "    best_q = float(\"-inf\")\n",
    "\n",
    "    action_values = []\n",
    "    for a in AVAILABLE_ACTIONS[s]:\n",
    "        trans = TRANSITIONS[(s, a)]\n",
    "        q_value = sum(\n",
    "            prob * (reward(s, a, s_next) + gamma * pyo.value(model.V[s_next]))\n",
    "            for s_next, prob in trans.items()\n",
    "        )\n",
    "        action_values.append((a, q_value))\n",
    "        if q_value > best_q:\n",
    "            best_q = q_value\n",
    "            best_action = a\n",
    "\n",
    "    # Show all Q-values for this state\n",
    "    q_str = \", \".join(f\"{a.name}={q:.3f}\" for a, q in action_values)\n",
    "    print(f\"  Ï€({s.name:8}) = {best_action.name:12}  [Q-values: {q_str}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de8f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
