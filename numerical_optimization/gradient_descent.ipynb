{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d922de9",
   "metadata": {},
   "source": [
    "# Gradient Descent for Unconstraint Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13dfe400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from chex import Array\n",
    "from jax import grad, jit\n",
    "\n",
    "\n",
    "A = jnp.array([[4.0, 1.0], [1.0, 3.0]])\n",
    "b = jnp.array([1.0, 2.0])\n",
    "\n",
    "\n",
    "def f(x: Array) -> Array:\n",
    "    \"\"\"Quadratic function f(x) = 0.5 * x^T A x - b^T x.\"\"\"\n",
    "    return 0.5 * jnp.dot(x, jnp.dot(A, x)) - jnp.dot(b, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cffc14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gradient function using JAX\n",
    "grad_f = jit(grad(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1b58f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 43 iterations.\n",
      "Optimal solution: [0.09091125 0.6363601 ]\n",
      "Function value at optimal solution: -0.6818182\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "max_iters = 100\n",
    "tol = 1e-6\n",
    "\n",
    "# Initial guess\n",
    "x = jnp.zeros_like(b)\n",
    "\n",
    "# Gradient descent loop\n",
    "for i in range(max_iters):\n",
    "    grad_value = grad_f(x)\n",
    "    x_new = x - learning_rate * grad_value\n",
    "\n",
    "    # Check for convergence\n",
    "    if jnp.linalg.norm(x_new - x) < tol:\n",
    "        print(f\"Converged after {i} iterations.\")\n",
    "        break\n",
    "\n",
    "    x = x_new\n",
    "\n",
    "print(\"Optimal solution:\", x)\n",
    "print(\"Function value at optimal solution:\", f(x))\n",
    "# Converged after 43 iterations.\n",
    "# Optimal solution: [0.09091125 0.6363601 ]\n",
    "# Function value at optimal solution: -0.6818182"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7797ef93",
   "metadata": {},
   "source": [
    "# Optimization with Constraints: Penalty Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24173c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from chex import Array\n",
    "from jax import grad, jit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "A = jnp.array([[4.0, 1.0], [1.0, 3.0]])\n",
    "b = jnp.array([1.0, 2.0])\n",
    "\n",
    "\n",
    "def f(x: Array) -> Array:\n",
    "    \"\"\"Quadratic function f(x) = 0.5 * x^T A x - b^T x.\"\"\"\n",
    "    return 0.5 * jnp.dot(x, jnp.dot(A, x)) - jnp.dot(b, x)\n",
    "\n",
    "\n",
    "def g(x: Array) -> Array:\n",
    "    \"\"\"Equality constraint: x1 + x2 = 1\"\"\"\n",
    "    return x[0] + x[1] - 1.0\n",
    "\n",
    "\n",
    "def h(x: Array) -> Array:\n",
    "    \"\"\"Inequality constraint: x1 >= 0.2, rewritten as -x1 + 0.2 <= 0\"\"\"\n",
    "    return -x[0] + 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c81c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_rho(x: Array, rho: float) -> Array:\n",
    "    \"\"\"Penalty function using quadratic penalties\"\"\"\n",
    "    eq_penalty = g(x) ** 2  # Equality constraint penalty\n",
    "    ineq_penalty = jnp.maximum(0, h(x)) ** 2  # Inequality constraint penalty\n",
    "    return f(x) + rho * (eq_penalty + ineq_penalty)\n",
    "\n",
    "\n",
    "@jit\n",
    "def grad_f_rho(x: Array, rho: float) -> Array:\n",
    "    return grad(lambda x: f_rho(x, rho))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b93a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 181 iterations with rho = 10.0.\n",
      "Optimal solution: [0.19789854 0.77583164]\n",
      "Function value at optimal solution: -0.61\n",
      "Equality constraint violation: 0.03\n",
      "Inequality constraint violation: 0.00\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01  # Smaller learning rate for stability\n",
    "max_iters = 1000\n",
    "tol = 1e-8\n",
    "rho = 10.0\n",
    "\n",
    "\n",
    "x = jnp.array([0.3, 0.7])  # Initial guess\n",
    "\n",
    "for i in range(max_iters):\n",
    "    grad_value = grad_f_rho(x, rho)\n",
    "    x_new = x - learning_rate * grad_value\n",
    "\n",
    "    if jnp.linalg.norm(x_new - x) < tol:\n",
    "        break\n",
    "    x = x_new\n",
    "\n",
    "# Calculate final metrics\n",
    "eq_violation = abs(float(g(x)))\n",
    "ineq_violation = max(0, float(h(x)))\n",
    "\n",
    "print(f\"Converged after {i} iterations with rho = {rho}.\")\n",
    "print(f\"Optimal solution: {x}\")\n",
    "print(f\"Function value at optimal solution: {f(x):.2f}\")\n",
    "print(f\"Equality constraint violation: {eq_violation:.2f}\")\n",
    "print(f\"Inequality constraint violation: {ineq_violation:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0981d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
