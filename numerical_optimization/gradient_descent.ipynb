{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d922de9",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13dfe400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from chex import Array\n",
    "from jax import grad, jit\n",
    "\n",
    "\n",
    "A = jnp.array([[4.0, 1.0], [1.0, 3.0]])\n",
    "b = jnp.array([1.0, 2.0])\n",
    "\n",
    "\n",
    "def f(x: Array) -> Array:\n",
    "    \"\"\"Quadratic function f(x) = 0.5 * x^T A x - b^T x.\"\"\"\n",
    "    return 0.5 * jnp.dot(x, jnp.dot(A, x)) - jnp.dot(b, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffc14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gradient function using JAX\n",
    "grad_f = jit(grad(f)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b58f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 43 iterations.\n",
      "Optimal solution: [0.09091125 0.6363601 ]\n",
      "Function value at optimal solution: -0.6818182\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "max_iters = 100\n",
    "tol = 1e-6\n",
    "\n",
    "# Initial guess\n",
    "x = jnp.zeros_like(b)\n",
    "\n",
    "# Gradient descent loop\n",
    "for i in range(max_iters):\n",
    "    grad_value = grad_f(x)\n",
    "    x_new = x - learning_rate * grad_value\n",
    "    \n",
    "    # Check for convergence\n",
    "    if jnp.linalg.norm(x_new - x) < tol:\n",
    "        print(f\"Converged after {i} iterations.\")\n",
    "        break\n",
    "    \n",
    "    x = x_new\n",
    "\n",
    "print(\"Optimal solution:\", x)\n",
    "print(\"Function value at optimal solution:\", f(x))\n",
    "# Converged after 43 iterations.\n",
    "# Optimal solution: [0.09091125 0.6363601 ]\n",
    "# Function value at optimal solution: -0.6818182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7aaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7797ef93",
   "metadata": {},
   "source": [
    "# Optimization with Constraints: Penalty Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b93a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving constrained optimization problem:\n",
      "min f(x) = 0.5 * x^T A x - b^T x\n",
      "s.t. x1 + x2 = 1\n",
      "     x1 >= 0.2\n",
      "\n",
      "==================================================\n",
      "\n",
      "Outer iteration 1, rho = 0.10\n",
      "  Solution: x = [0.105090, 0.648057]\n",
      "  Objective value: f(x) = -0.681045\n",
      "  Equality constraint violation: |g(x)| = 2.47e-01\n",
      "  Inequality constraint violation: max(0, h(x)) = 9.49e-02\n",
      "  Converged: True (iterations: 34)\n",
      "\n",
      "Outer iteration 2, rho = 0.20\n",
      "  Solution: x = [0.116476, 0.657869]\n",
      "  Objective value: f(x) = -0.679267\n",
      "  Equality constraint violation: |g(x)| = 2.26e-01\n",
      "  Inequality constraint violation: max(0, h(x)) = 8.35e-02\n",
      "  Converged: True (iterations: 21)\n",
      "\n",
      "Outer iteration 3, rho = 0.40\n",
      "  Solution: x = [0.133505, 0.673575]\n",
      "  Objective value: f(x) = -0.674527\n",
      "  Equality constraint violation: |g(x)| = 1.93e-01\n",
      "  Inequality constraint violation: max(0, h(x)) = 6.65e-02\n",
      "  Converged: True (iterations: 27)\n",
      "\n",
      "Outer iteration 4, rho = 0.80\n",
      "  Solution: x = [0.154497, 0.695263]\n",
      "  Objective value: f(x) = -0.664783\n",
      "  Equality constraint violation: |g(x)| = 1.50e-01\n",
      "  Inequality constraint violation: max(0, h(x)) = 4.55e-02\n",
      "  Converged: True (iterations: 31)\n",
      "\n",
      "Outer iteration 5, rho = 1.60\n",
      "  Solution: x = [0.174399, 0.720551]\n",
      "  Objective value: f(x) = -0.650217\n",
      "  Equality constraint violation: |g(x)| = 1.05e-01\n",
      "  Inequality constraint violation: max(0, h(x)) = 2.56e-02\n",
      "  Converged: True (iterations: 23)\n",
      "\n",
      "Outer iteration 6, rho = 3.20\n",
      "  Solution: x = [0.188378, 0.745308]\n",
      "  Objective value: f(x) = -0.634396\n",
      "  Equality constraint violation: |g(x)| = 6.63e-02\n",
      "  Inequality constraint violation: max(0, h(x)) = 1.16e-02\n",
      "  Converged: True (iterations: 15)\n",
      "\n",
      "Outer iteration 7, rho = 6.40\n",
      "  Solution: x = [0.195670, 0.765805]\n",
      "  Objective value: f(x) = -0.621175\n",
      "  Equality constraint violation: |g(x)| = 3.85e-02\n",
      "  Inequality constraint violation: max(0, h(x)) = 4.33e-03\n",
      "  Converged: True (iterations: 38)\n",
      "\n",
      "Outer iteration 8, rho = 12.80\n",
      "  Solution: x = [0.198622, 0.780302]\n",
      "  Objective value: f(x) = -0.612033\n",
      "  Equality constraint violation: |g(x)| = 2.11e-02\n",
      "  Inequality constraint violation: max(0, h(x)) = 1.38e-03\n",
      "  Converged: True (iterations: 91)\n",
      "\n",
      "Outer iteration 9, rho = 25.60\n",
      "  Solution: x = [0.199606, 0.789306]\n",
      "  Objective value: f(x) = -0.606477\n",
      "  Equality constraint violation: |g(x)| = 1.11e-02\n",
      "  Inequality constraint violation: max(0, h(x)) = 3.94e-04\n",
      "  Converged: True (iterations: 40)\n",
      "\n",
      "Outer iteration 10, rho = 51.20\n",
      "  Solution: x = [0.199895, 0.794407]\n",
      "  Objective value: f(x) = -0.603371\n",
      "  Equality constraint violation: |g(x)| = 5.70e-03\n",
      "  Inequality constraint violation: max(0, h(x)) = 1.05e-04\n",
      "  Converged: True (iterations: 44)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Final solution:\n",
      "x* = [0.199895, 0.794407]\n",
      "f(x*) = -0.603371\n",
      "g(x*) = -0.005698 (should be 0)\n",
      "h(x*) = 0.000105 (should be <= 0)\n",
      "\n",
      "Constraint satisfaction:\n",
      "x1 + x2 = 0.994302 (should be 1.0)\n",
      "x1 = 0.199895 (should be >= 0.2)\n",
      "\n",
      "==================================================\n",
      "\n",
      "Comparison with unconstrained solution:\n",
      "Unconstrained: x = [0.090909, 0.636364], f(x) = -0.681818\n",
      "Constrained:   x = [0.199895, 0.794407], f(x) = -0.603371\n",
      "\n",
      "==================================================\n",
      "\n",
      "Analytical verification:\n",
      "At the optimal point, the inequality constraint should be active (x1 = 0.2)\n",
      "Combined with x1 + x2 = 1, we get x2 = 0.8\n",
      "Analytical: x = [0.200000, 0.800000], f(x) = -0.600000\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from chex import Array\n",
    "from jax import grad, jit\n",
    "\n",
    "# Original objective function components\n",
    "A = jnp.array([[4.0, 1.0], [1.0, 3.0]])\n",
    "b = jnp.array([1.0, 2.0])\n",
    "\n",
    "def f(x: Array) -> Array:\n",
    "    \"\"\"Quadratic function f(x) = 0.5 * x^T A x - b^T x.\"\"\"\n",
    "    return 0.5 * jnp.dot(x, jnp.dot(A, x)) - jnp.dot(b, x)\n",
    "\n",
    "# Define constraints\n",
    "def g(x: Array) -> Array:\n",
    "    \"\"\"Equality constraint: x1 + x2 = 1\"\"\"\n",
    "    return x[0] + x[1] - 1.0\n",
    "\n",
    "def h(x: Array) -> Array:\n",
    "    \"\"\"Inequality constraint: x1 >= 0.2, rewritten as -x1 + 0.2 <= 0\"\"\"\n",
    "    return -x[0] + 0.2\n",
    "\n",
    "# Penalty function\n",
    "def penalty(x: Array, rho: float) -> Array:\n",
    "    \"\"\"Penalty function using quadratic penalties\"\"\"\n",
    "    eq_penalty = g(x)**2  # Equality constraint penalty\n",
    "    ineq_penalty = jnp.maximum(0, h(x))**2  # Inequality constraint penalty\n",
    "    return f(x) + rho * (eq_penalty + ineq_penalty)\n",
    "\n",
    "# Create gradient function for penalized objective\n",
    "@jit\n",
    "def grad_penalty(x: Array, rho: float) -> Array:\n",
    "    return grad(lambda x: penalty(x, rho))(x)\n",
    "\n",
    "# Solve with penalty method - improved version\n",
    "def solve_constrained():\n",
    "    # Parameters - adjusted for better stability\n",
    "    max_iters = 500  # More iterations per rho\n",
    "    tol = 1e-8\n",
    "    \n",
    "    # Penalty method parameters - more gradual increase\n",
    "    rho = 0.1  # Start with smaller penalty\n",
    "    rho_multiplier = 2.0  # Smaller increase factor\n",
    "    max_outer_iters = 10  # More outer iterations\n",
    "    \n",
    "    # Initial guess - closer to feasible region\n",
    "    x = jnp.array([0.3, 0.7])\n",
    "    \n",
    "    print(\"Solving constrained optimization problem:\")\n",
    "    print(\"min f(x) = 0.5 * x^T A x - b^T x\")\n",
    "    print(\"s.t. x1 + x2 = 1\")\n",
    "    print(\"     x1 >= 0.2\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    best_x = x\n",
    "    best_obj = jnp.inf\n",
    "    \n",
    "    # Outer loop: gradually increase penalty parameter\n",
    "    for outer_iter in range(max_outer_iters):\n",
    "        print(f\"Outer iteration {outer_iter + 1}, rho = {rho:.2f}\")\n",
    "        \n",
    "        # Adaptive learning rate based on rho\n",
    "        learning_rate = min(0.1, 1.0 / (1.0 + rho))\n",
    "        \n",
    "        # Inner loop: gradient descent for fixed rho\n",
    "        converged = False\n",
    "        for i in range(max_iters):\n",
    "            grad_value = grad_penalty(x, rho)\n",
    "            \n",
    "            # Line search for better step size\n",
    "            alpha = learning_rate\n",
    "            for _ in range(10):\n",
    "                x_new = x - alpha * grad_value\n",
    "                if penalty(x_new, rho) < penalty(x, rho):\n",
    "                    break\n",
    "                alpha *= 0.5\n",
    "            \n",
    "            x_new = x - alpha * grad_value\n",
    "            \n",
    "            # Check for convergence\n",
    "            if jnp.linalg.norm(x_new - x) < tol:\n",
    "                converged = True\n",
    "                break\n",
    "            \n",
    "            x = x_new\n",
    "        \n",
    "        # Check for NaN\n",
    "        if jnp.any(jnp.isnan(x)):\n",
    "            print(\"  WARNING: NaN detected, reverting to best solution\")\n",
    "            x = best_x\n",
    "            break\n",
    "        \n",
    "        # Print current solution and constraint violations\n",
    "        eq_violation = abs(g(x))\n",
    "        ineq_violation = max(0, float(h(x)))\n",
    "        obj_value = float(f(x))\n",
    "        \n",
    "        print(f\"  Solution: x = [{x[0]:.6f}, {x[1]:.6f}]\")\n",
    "        print(f\"  Objective value: f(x) = {obj_value:.6f}\")\n",
    "        print(f\"  Equality constraint violation: |g(x)| = {eq_violation:.2e}\")\n",
    "        print(f\"  Inequality constraint violation: max(0, h(x)) = {ineq_violation:.2e}\")\n",
    "        print(f\"  Converged: {converged} (iterations: {i+1})\")\n",
    "        print()\n",
    "        \n",
    "        # Update best solution if constraints are better satisfied\n",
    "        if eq_violation + ineq_violation < 0.01 and obj_value < best_obj:\n",
    "            best_x = x.copy()\n",
    "            best_obj = obj_value\n",
    "        \n",
    "        # Check if constraints are satisfied\n",
    "        if eq_violation < 1e-5 and ineq_violation < 1e-5:\n",
    "            print(\"Constraints satisfied!\")\n",
    "            break\n",
    "        \n",
    "        # Increase penalty parameter\n",
    "        rho *= rho_multiplier\n",
    "    \n",
    "    # Use best solution found\n",
    "    x = best_x\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    print(\"Final solution:\")\n",
    "    print(f\"x* = [{x[0]:.6f}, {x[1]:.6f}]\")\n",
    "    print(f\"f(x*) = {f(x):.6f}\")\n",
    "    print(f\"g(x*) = {g(x):.6f} (should be 0)\")\n",
    "    print(f\"h(x*) = {h(x):.6f} (should be <= 0)\")\n",
    "    \n",
    "    # Verify constraints\n",
    "    print(f\"\\nConstraint satisfaction:\")\n",
    "    print(f\"x1 + x2 = {x[0] + x[1]:.6f} (should be 1.0)\")\n",
    "    print(f\"x1 = {x[0]:.6f} (should be >= 0.2)\")\n",
    "    \n",
    "    # Compare with unconstrained solution\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    print(\"Comparison with unconstrained solution:\")\n",
    "    \n",
    "    # Solve unconstrained problem\n",
    "    x_unconstrained = jnp.zeros_like(b)\n",
    "    grad_f = jit(grad(f))\n",
    "    \n",
    "    for i in range(100):\n",
    "        grad_value = grad_f(x_unconstrained)\n",
    "        x_new = x_unconstrained - 0.1 * grad_value\n",
    "        if jnp.linalg.norm(x_new - x_unconstrained) < tol:\n",
    "            break\n",
    "        x_unconstrained = x_new\n",
    "    \n",
    "    print(f\"Unconstrained: x = [{x_unconstrained[0]:.6f}, {x_unconstrained[1]:.6f}], f(x) = {f(x_unconstrained):.6f}\")\n",
    "    print(f\"Constrained:   x = [{x[0]:.6f}, {x[1]:.6f}], f(x) = {f(x):.6f}\")\n",
    "    \n",
    "    # Analytical check - for this problem we can verify the solution\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    print(\"Analytical verification:\")\n",
    "    print(\"At the optimal point, the inequality constraint should be active (x1 = 0.2)\")\n",
    "    print(\"Combined with x1 + x2 = 1, we get x2 = 0.8\")\n",
    "    x_analytical = jnp.array([0.2, 0.8])\n",
    "    print(f\"Analytical: x = [{x_analytical[0]:.6f}, {x_analytical[1]:.6f}], f(x) = {f(x_analytical):.6f}\")\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Run the constrained optimization\n",
    "optimal_x = solve_constrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593fcbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
