{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8bbcc4",
   "metadata": {},
   "source": [
    "# MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706c1c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "from enum import IntEnum\n",
    "\n",
    "\n",
    "# Define the state space, action space, and other MDP components\n",
    "class State(IntEnum):\n",
    "    S0 = 0\n",
    "    W = 1\n",
    "    M = 2\n",
    "    WM = 3\n",
    "    SUCCESS = 4\n",
    "    ABANDON = 5\n",
    "\n",
    "\n",
    "class Action(IntEnum):\n",
    "    ASK_WEATHER = 0\n",
    "    ASK_MOOD = 1\n",
    "    ASK_BOTH = 2\n",
    "    RECOMMEND = 3\n",
    "\n",
    "\n",
    "TERMINAL_STATES = {State.SUCCESS, State.ABANDON}\n",
    "\n",
    "AVAILABLE_ACTIONS = {\n",
    "    State.S0: [Action.ASK_WEATHER, Action.ASK_MOOD, Action.ASK_BOTH],\n",
    "    State.W: [Action.ASK_MOOD],\n",
    "    State.M: [Action.ASK_WEATHER],\n",
    "    State.WM: [Action.RECOMMEND],\n",
    "    State.SUCCESS: [],\n",
    "    State.ABANDON: [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2839549",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSITIONS = {\n",
    "    (State.S0, Action.ASK_WEATHER): {\n",
    "        State.W: 0.75,\n",
    "        State.S0: 0.15,\n",
    "        State.ABANDON: 0.10,\n",
    "    },\n",
    "    (State.S0, Action.ASK_MOOD): {State.M: 0.70, State.S0: 0.20, State.ABANDON: 0.10},\n",
    "    (State.S0, Action.ASK_BOTH): {\n",
    "        State.WM: 0.50,\n",
    "        State.W: 0.20,\n",
    "        State.M: 0.15,\n",
    "        State.ABANDON: 0.15,\n",
    "    },\n",
    "    (State.W, Action.ASK_MOOD): {State.WM: 0.80, State.W: 0.10, State.ABANDON: 0.10},\n",
    "    (State.M, Action.ASK_WEATHER): {State.WM: 0.80, State.M: 0.10, State.ABANDON: 0.10},\n",
    "    (State.WM, Action.RECOMMEND): {State.SUCCESS: 0.75, State.ABANDON: 0.25},\n",
    "}\n",
    "\n",
    "TransitionModel = Callable[[State, Action], Dict[State, float]]\n",
    "\n",
    "\n",
    "def transition_model(state: State, action: Action) -> Dict[State, float]:\n",
    "    \"\"\"Return the transition probabilities for a given state and action.\"\"\"\n",
    "    if (state, action) not in TRANSITIONS:\n",
    "        raise ValueError(f\"Invalid state-action pair: ({state}, {action})\")\n",
    "    return TRANSITIONS[(state, action)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f4bb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "RewardModel = Callable[[State, Action, State], float]\n",
    "\n",
    "\n",
    "def reward_model(state: State, action: Action, next_state: State) -> float:\n",
    "    \"\"\"Return the reward for a given state, action, and next state.\"\"\"\n",
    "    return 10.0 if next_state == State.SUCCESS else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b76d00c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdp_step(\n",
    "    state: State, action: Action, TM: TransitionModel, RM: RewardModel\n",
    ") -> Tuple[State, float]:\n",
    "    \"\"\"Sample next state and compute reward.\"\"\"\n",
    "    P = TM(state, action)\n",
    "    next_state = random.choices(list(P.keys()), weights=list(P.values()))[0]\n",
    "    return next_state, RM(state, action, next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dadd5180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: S0, Action: ASK_WEATHER, Reward: 0.0, Next: W\n",
      "State: W, Action: ASK_MOOD, Reward: 0.0, Next: WM\n",
      "State: WM, Action: RECOMMEND, Reward: 10.0, Next: SUCCESS\n",
      "\n",
      "State: S0, Action: ASK_MOOD, Reward: 0.0, Next: S0\n",
      "State: S0, Action: ASK_MOOD, Reward: 0.0, Next: S0\n",
      "State: S0, Action: ASK_BOTH, Reward: 0.0, Next: W\n",
      "State: W, Action: ASK_MOOD, Reward: 0.0, Next: WM\n",
      "State: WM, Action: RECOMMEND, Reward: 10.0, Next: SUCCESS\n",
      "\n",
      "State: S0, Action: ASK_MOOD, Reward: 0.0, Next: M\n",
      "State: M, Action: ASK_WEATHER, Reward: 0.0, Next: WM\n",
      "State: WM, Action: RECOMMEND, Reward: 10.0, Next: SUCCESS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def random_policy(state: State) -> Action:\n",
    "    \"\"\"Select a random action from the available actions for the given state.\"\"\"\n",
    "    return random.choice(AVAILABLE_ACTIONS[state])\n",
    "\n",
    "\n",
    "def sample_trajectory(\n",
    "    policy: Callable[[State], Action] = random_policy,\n",
    "    num_steps: int = 1000,\n",
    ") -> List[List[Tuple[State, Action, float, State]]]:\n",
    "    \"\"\"Sample a trajectory, resetting to S0 when reaching terminal states.\"\"\"\n",
    "    trajectories = []\n",
    "    state = State.S0\n",
    "\n",
    "    trajectory = []\n",
    "    for _ in range(num_steps):\n",
    "        if state in TERMINAL_STATES:\n",
    "            # Reset to initial state and append to trajectory\n",
    "            trajectories.append(trajectory)\n",
    "            trajectory = []\n",
    "            state = State.S0\n",
    "\n",
    "        action = random_policy(state)\n",
    "        next_state, reward = mdp_step(state, action, transition_model, reward_model)\n",
    "        trajectory.append((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "\n",
    "    return trajectories\n",
    "\n",
    "\n",
    "trajectories = sample_trajectory(num_steps=1000)\n",
    "\n",
    "for trajectory in trajectories[:3]:\n",
    "    for state, action, reward, next_state in trajectory:\n",
    "        print(\n",
    "            f\"State: {state.name}, Action: {action.name}, Reward: {reward}, Next: {next_state.name}\"\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7516e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc545b5a",
   "metadata": {},
   "source": [
    "# Estimating the Transition Model and the Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb86ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import SVI, Trace_ELBO, autoguide\n",
    "import optax\n",
    "\n",
    "# Define valid state-action pairs and their possible next states\n",
    "VALID_SA_PAIRS = list(TRANSITIONS.keys())\n",
    "SA_TO_IDX = {sa: i for i, sa in enumerate(VALID_SA_PAIRS)}\n",
    "\n",
    "# For each valid (s, a), store the possible next states\n",
    "SA_NEXT_STATES = {sa: list(TRANSITIONS[sa].keys()) for sa in VALID_SA_PAIRS}\n",
    "\n",
    "# Build mapping from (s, a, s') -> unique index for rewards\n",
    "SAS_TRIPLES = []\n",
    "SAS_TO_IDX = {}\n",
    "for sa in VALID_SA_PAIRS:\n",
    "    s, a = sa\n",
    "    for ns in SA_NEXT_STATES[sa]:\n",
    "        SAS_TO_IDX[(s, a, ns)] = len(SAS_TRIPLES)\n",
    "        SAS_TRIPLES.append((s, a, ns))\n",
    "\n",
    "# Number of states (for transition matrix columns)\n",
    "NUM_STATES = len(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14353f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transitions: 999\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(trajectories):\n",
    "    \"\"\"\n",
    "    Preprocess trajectories into vectorized form for efficient inference.\n",
    "\n",
    "    Returns:\n",
    "        sa_indices: Index of the (s,a) pair for each transition\n",
    "        next_state_indices: State index of s' (direct state value)\n",
    "        sas_indices: Index of the (s,a,s') triple for each transition\n",
    "        rewards: Reward values\n",
    "    \"\"\"\n",
    "    sa_indices = []\n",
    "    next_state_indices = []\n",
    "    sas_indices = []\n",
    "    rewards = []\n",
    "\n",
    "    for trajectory in trajectories:\n",
    "        for state, action, reward, next_state in trajectory:\n",
    "            sa = (state, action)\n",
    "            sa_idx = SA_TO_IDX[sa]\n",
    "            sas_idx = SAS_TO_IDX[(state, action, next_state)]\n",
    "\n",
    "            sa_indices.append(sa_idx)\n",
    "            next_state_indices.append(int(next_state))\n",
    "            sas_indices.append(sas_idx)\n",
    "            rewards.append(float(reward))\n",
    "\n",
    "    return (\n",
    "        jnp.array(sa_indices),\n",
    "        jnp.array(next_state_indices),\n",
    "        jnp.array(sas_indices),\n",
    "        jnp.array(rewards),\n",
    "    )\n",
    "\n",
    "\n",
    "sa_indices, next_state_indices, sas_indices, rewards_data = preprocess_data(\n",
    "    trajectories\n",
    ")\n",
    "print(f\"Number of transitions: {len(sa_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "181d0b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_SIZES = [len(SA_NEXT_STATES[sa]) for sa in VALID_SA_PAIRS]\n",
    "\n",
    "\n",
    "def mdp_model(sa_indices, next_state_indices, sas_indices, rewards):\n",
    "    \"\"\"\n",
    "    Probabilistic model for MDP learning.\n",
    "\n",
    "    Transition model:\n",
    "        θ_{s,a} ~ Dirichlet(1)\n",
    "        s' | s, a ~ Categorical(θ_{s,a})\n",
    "\n",
    "    Reward model:\n",
    "        μ_{s,a,s'} ~ Normal(0, 10)\n",
    "        σ ~ HalfNormal(1)\n",
    "        r | s, a, s' ~ Normal(μ_{s,a,s'}, σ)\n",
    "    \"\"\"\n",
    "    num_sas = len(SAS_TRIPLES)\n",
    "\n",
    "    # Transition probabilities\n",
    "    trans_probs_matrix = jnp.zeros((len(VALID_SA_PAIRS), NUM_STATES))\n",
    "\n",
    "    for i, sa in enumerate(VALID_SA_PAIRS):\n",
    "        num_next = SA_SIZES[i]\n",
    "        probs = numpyro.sample(f\"trans_prob_{i}\", dist.Dirichlet(jnp.ones(num_next)))\n",
    "\n",
    "        # Place probabilities into correct state columns\n",
    "        next_state_ids = jnp.array([int(s) for s in SA_NEXT_STATES[sa]])\n",
    "        trans_probs_matrix = trans_probs_matrix.at[i, next_state_ids].set(probs)\n",
    "\n",
    "    # Reward model parameters\n",
    "    reward_means = numpyro.sample(\"reward_means\", dist.Normal(0, 10).expand([num_sas]))\n",
    "    reward_noise = numpyro.sample(\"reward_noise\", dist.HalfNormal(1.0))\n",
    "\n",
    "    # Likelihoods\n",
    "    with numpyro.plate(\"data\", len(sa_indices)):\n",
    "        # Transition likelihood\n",
    "        obs_probs = trans_probs_matrix[sa_indices]\n",
    "        numpyro.sample(\n",
    "            \"obs_next_state\", dist.Categorical(probs=obs_probs), obs=next_state_indices\n",
    "        )\n",
    "\n",
    "        # Reward likelihood\n",
    "        obs_reward_means = reward_means[sas_indices]\n",
    "        numpyro.sample(\n",
    "            \"obs_reward\", dist.Normal(obs_reward_means, reward_noise), obs=rewards\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da496ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SVI inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 919.25it/s, init loss: 4405.4521, avg. loss [1901-2000]: -4401.1343] \n"
     ]
    }
   ],
   "source": [
    "# Run inference using SVI (faster than MCMC for this model)\n",
    "rng_key = jax.random.PRNGKey(0)\n",
    "\n",
    "print(\"Running SVI inference...\")\n",
    "guide = autoguide.AutoNormal(mdp_model)\n",
    "optimizer = optax.adam(0.01)\n",
    "svi = SVI(mdp_model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "# Run optimization\n",
    "svi_result = svi.run(\n",
    "    rng_key,\n",
    "    num_steps=2000,\n",
    "    sa_indices=sa_indices,\n",
    "    next_state_indices=next_state_indices,\n",
    "    sas_indices=sas_indices,\n",
    "    rewards=rewards_data,\n",
    "    progress_bar=True,\n",
    ")\n",
    "\n",
    "# Get posterior samples from the guide\n",
    "rng_key, sample_key = jax.random.split(rng_key)\n",
    "predictive = numpyro.infer.Predictive(guide, params=svi_result.params, num_samples=1000)\n",
    "samples = predictive(\n",
    "    sample_key, sa_indices, next_state_indices, sas_indices, rewards_data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f6189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LEARNED TRANSITION PROBABILITIES\n",
      "============================================================\n",
      "\n",
      "P(s' | s=S0, a=ASK_WEATHER):\n",
      "          True       Learned       Std\n",
      "         W:    0.750         0.735    0.0375\n",
      "        S0:    0.150         0.170    0.0339\n",
      "   ABANDON:    0.100         0.095    0.0268\n",
      "\n",
      "P(s' | s=S0, a=ASK_MOOD):\n",
      "          True       Learned       Std\n",
      "         M:    0.700         0.708    0.0371\n",
      "        S0:    0.200         0.186    0.0310\n",
      "   ABANDON:    0.100         0.106    0.0241\n",
      "\n",
      "P(s' | s=S0, a=ASK_BOTH):\n",
      "          True       Learned       Std\n",
      "        WM:    0.500         0.475    0.0433\n",
      "         W:    0.200         0.232    0.0343\n",
      "         M:    0.150         0.149    0.0284\n",
      "   ABANDON:    0.150         0.144    0.0280\n",
      "\n",
      "P(s' | s=W, a=ASK_MOOD):\n",
      "          True       Learned       Std\n",
      "        WM:    0.800         0.755    0.0349\n",
      "         W:    0.100         0.125    0.0263\n",
      "   ABANDON:    0.100         0.120    0.0261\n",
      "\n",
      "P(s' | s=M, a=ASK_WEATHER):\n",
      "          True       Learned       Std\n",
      "        WM:    0.800         0.800    0.0292\n",
      "         M:    0.100         0.098    0.0232\n",
      "   ABANDON:    0.100         0.102    0.0235\n",
      "\n",
      "P(s' | s=WM, a=RECOMMEND):\n",
      "          True       Learned       Std\n",
      "   SUCCESS:    0.750         0.776    0.0247\n",
      "   ABANDON:    0.250         0.224    0.0247\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LEARNED TRANSITION PROBABILITIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, sa in enumerate(VALID_SA_PAIRS):\n",
    "    s, a = sa\n",
    "    possible_next = SA_NEXT_STATES[sa]\n",
    "\n",
    "    probs_samples = samples[f\"trans_prob_{i}\"]\n",
    "    probs_mean = jnp.mean(probs_samples, axis=0)\n",
    "    probs_std = jnp.std(probs_samples, axis=0)\n",
    "\n",
    "    print(f\"\\nP(s' | s={State(s).name}, a={Action(a).name}):\")\n",
    "    print(f\"  {'True':>12}  {'Learned':>12}  {'Std':>8}\")\n",
    "\n",
    "    true_probs = TRANSITIONS[sa]\n",
    "    for idx, ns in enumerate(possible_next):\n",
    "        true_p = true_probs.get(ns, 0.0)\n",
    "        learned_p = float(probs_mean[idx])\n",
    "        std_p = float(probs_std[idx])\n",
    "        print(\n",
    "            f\"  {State(ns).name:>8}: {true_p:>8.3f}  {learned_p:>12.3f}  {std_p:>8.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6872f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LEARNED REWARD PARAMETERS\n",
      "============================================================\n",
      "Shared reward noise σ: 0.0012\n",
      "\n",
      "R(s=S0, a=ASK_WEATHER, s'=W): True=0.0, Learned μ=-0.001 (±0.000)\n",
      "R(s=S0, a=ASK_WEATHER, s'=S0): True=0.0, Learned μ=0.002 (±0.000)\n",
      "R(s=S0, a=ASK_WEATHER, s'=ABANDON): True=0.0, Learned μ=0.001 (±0.001)\n",
      "R(s=S0, a=ASK_MOOD, s'=M): True=0.0, Learned μ=0.000 (±0.000)\n",
      "R(s=S0, a=ASK_MOOD, s'=S0): True=0.0, Learned μ=0.001 (±0.000)\n",
      "R(s=S0, a=ASK_MOOD, s'=ABANDON): True=0.0, Learned μ=-0.001 (±0.000)\n",
      "R(s=S0, a=ASK_BOTH, s'=WM): True=0.0, Learned μ=-0.001 (±0.000)\n",
      "R(s=S0, a=ASK_BOTH, s'=W): True=0.0, Learned μ=0.001 (±0.000)\n",
      "R(s=S0, a=ASK_BOTH, s'=M): True=0.0, Learned μ=-0.001 (±0.000)\n",
      "R(s=S0, a=ASK_BOTH, s'=ABANDON): True=0.0, Learned μ=-0.001 (±0.000)\n",
      "R(s=W, a=ASK_MOOD, s'=WM): True=0.0, Learned μ=0.000 (±0.000)\n",
      "R(s=W, a=ASK_MOOD, s'=W): True=0.0, Learned μ=0.002 (±0.000)\n",
      "R(s=W, a=ASK_MOOD, s'=ABANDON): True=0.0, Learned μ=0.000 (±0.000)\n",
      "R(s=M, a=ASK_WEATHER, s'=WM): True=0.0, Learned μ=0.001 (±0.000)\n",
      "R(s=M, a=ASK_WEATHER, s'=M): True=0.0, Learned μ=0.001 (±0.000)\n",
      "R(s=M, a=ASK_WEATHER, s'=ABANDON): True=0.0, Learned μ=-0.001 (±0.000)\n",
      "R(s=WM, a=RECOMMEND, s'=SUCCESS): True=10.0, Learned μ=10.000 (±0.000)\n",
      "R(s=WM, a=RECOMMEND, s'=ABANDON): True=0.0, Learned μ=0.000 (±0.000)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LEARNED REWARD PARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "reward_means_samples = samples[\"reward_means\"]\n",
    "reward_noise_samples = samples[\"reward_noise\"]\n",
    "\n",
    "print(f\"Shared reward noise σ: {float(jnp.mean(reward_noise_samples)):.4f}\")\n",
    "print()\n",
    "\n",
    "for sa in VALID_SA_PAIRS:\n",
    "    s, a = sa\n",
    "    for ns in SA_NEXT_STATES[sa]:\n",
    "        sas_idx = SAS_TO_IDX[(s, a, ns)]\n",
    "\n",
    "        learned_mean = float(jnp.mean(reward_means_samples[:, sas_idx]))\n",
    "        mean_std = float(jnp.std(reward_means_samples[:, sas_idx]))\n",
    "\n",
    "        true_reward = reward_model(State(s), Action(a), State(ns))\n",
    "\n",
    "        print(\n",
    "            f\"R(s={State(s).name}, a={Action(a).name}, s'={State(ns).name}): \"\n",
    "            f\"True={true_reward:.1f}, Learned μ={learned_mean:.3f} (±{mean_std:.3f})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd38c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
